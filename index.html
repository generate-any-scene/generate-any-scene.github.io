<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="GenerateAnyScene">
  <meta name="keywords" content="Multimodal, Large Language Model, benchmark, synthetic data, dataset, Diffusion model, Text-to-image model">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Generate Any Scene</title>

  <!-- Google Tag Manager -->
  <script>(function (w, d, s, l, i) {
      w[l] = w[l] || []; w[l].push({
        'gtm.start':
          new Date().getTime(), event: 'gtm.js'
      }); var f = d.getElementsByTagName(s)[0],
        j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
          'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    })(window, document, 'script', 'dataLayer', 'GTM-NKDF32ZZ');</script>
  <!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <!-- MathJax -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.16.0/gradio.js"></script>
</head>

<body>
  <!-- Google Tag Manager (noscript) -->
  <!-- <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NKDF32ZZ" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript> -->
  <!-- End Google Tag Manager (noscript) -->

  <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://yushi-hu.github.io">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://zeyofu.github.io/blink/">
              BLINK
            </a>
            <a class="navbar-item" href="https://visual-program-distillation.github.io/">
              Visual Program Distillation (VPD)
            </a>
            <a class="navbar-item" href="https://finegrainedrlhf.github.io/">
              Fine-Grained RLHF
            </a>
            <a class="navbar-item" href="https://tifa-benchmark.github.io/">
              TIFA
            </a>
            <a class="navbar-item" href="https://yushi-hu.github.io/promptcap_demo/">
              PromptCap
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav> -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- <h1 class="title is-1 publication-title"><img src="static/images/icon.png" alt="icon"
                style="width:75px; transform: translateY(20%);"><span style="color: rgb(100, 138, 195);"><b>Task Me Anything</b></span>: <br> Sketching as a Visual Chain of Thought for Multimodal Language Models</h1> -->
            <h1 class="title is-1 publication-title"><img src="static/images/icon.png" alt="icon"
                style="width:75px; transform: translateY(20%);"><span style="color: rgb(15, 46, 93);"><b>Generate Any Scene
                  </b></span>: <br> Evaluating and Improving Text-to-Vision Generation with Scene Graph Programming</h1>
            <div class="is-size-5">
              <span class="author-block">
                <a href="https://weikaih04.github.io/" style="color:#f68946;font-weight:normal;">Ziqi Gao<sup>*</sup></a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://weikaih04.github.io/" style="color:#f68946;font-weight:normal;">Weikai Huang<sup>*</sup></a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://jieyuz2.github.io/" style="color:#f68946;font-weight:normal;">Jieyu Zhang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://anikem.github.io/" style="color:#008AD7;font-weight:normal;">Aniruddha Kembhavi</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="http://www.ranjaykrishna.com/" style="color:#f68946;font-weight:normal;">Ranjay Krishna</a><sup>1,2</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <b style="color:#f68946; font-weight:normal">▶ </b>University of Washington
              </span>
              <span class="author-block">
                <b style="color:#008AD7; font-weight:normal">▶ </b>Allen Institute for AI
              </span>
              <!-- <span class="author-block">
                <b style="color:#008AD7; font-weight:normal">▶ </b>University of Pennsylvania
              </span> -->
              <span class="author-block">
                &nbsp;&nbsp;<sup>*</sup>Equal Contribution
              </span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2406.11775" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                
                <!-- PDF Link. -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/pdf/2406.09403" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span> -->
                <!-- Video Link. -->

                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/RAIVNLab/GenerateAnyScene"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- <span class="link-block">
                  <a href="https://huggingface.co/spaces/zixianma/TaskMeAnything-UI"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Graphical Interface</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/collections/jieyuz2/taskmeanything-664ebf028ab2524c0380526a"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Huggingface Collection</span>
                  </a>
                </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
  </section>



  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop is-centered has-text-centered">
        <img src="./static/images/teaser.png">
        <br>
                <h2 class="subtitle has-text-left">
                    Generative models like DALL-E and Sora have gained attention by producing implausible images, such as “astronauts riding a horse in space.” Despite the proliferation of text-to-vision models that have inundated the internet with synthetic visuals, from images to 3D assets, current benchmarks predominantly evaluate these models on real-world scenes paired with captions. 
                </h2>
                <h2 class="subtitle has-text-left">
                    We introduce <strong>Generate Any Scene</strong>, a framework that systematically enumerates scene graphs representing a vast array of visual scenes, spanning realistic to imaginative compositions. Generate Any Scene leverages <strong>Scene Graph Programming:</strong> a method for dynamically constructing scene graphs of varying complexity from a structured taxonomy of visual elements. This taxonomy includes numerous objects, attributes, and relations, enabling the synthesis of an almost infinite variety of scene graphs. Using these structured representations, Generate Any Scene translates each scene graph into a caption, enabling scalable evaluation of text-to-vision models through standard metrics. 
                </h2>
                <h2 class="subtitle has-text-left">
                    We conduct extensive evaluations across multiple text-to-image, text-to-video, and text-to-3D models, presenting key findings on model performance. We find that DiT-backbone text-to-image models align more closely with input captions than UNet-backbone models. Text-to-video models struggle with balancing dynamics and consistency, while both text-to-video and text-to-3D models show notable gaps in human preference alignment.
                </h2>
                <h2 class="subtitle has-text-left">
                    Additionally, we demonstrate the effectiveness of Generate Any Scene by conducting three practical applications leveraging captions generated by Generate Any Scene:
                    <ol style="margin-left: 40px;">
                        <li>A self-improving framework where models iteratively enhance their performance using generated data.</li>
                        <li>A distillation process to transfer specific strengths from proprietary models to open-source counterparts.</li>
                        <li>Improvements in content moderation by identifying and generating challenging synthetic data.</li>
                    </ol>
                </h2>
            </div>
        </div>
    </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop is-centered has-text-centered">
        <h2 class="title is-3">Captions Generation Process</h2>
        <h2 class="subtitle has-text-left">
          
        <!-- <div class="content has-text-justified">
        <img src="static/images/website_examples_1.jpg">
        <img src="static/images/website_examples_2.jpg">
        <div class="content has-text-justified"> -->
        <img src="./static/images/generation_process.png">
        <h2 class="subtitle has-text-left">
          <b>A benchmark generation engine that generates benchmarks on-the-fly tailored to the user’s need for assessing multimodal language models like GPT-4o.</b>
          The top part illustrates the task generation process with an example video synthesized with 3D objects and their annotations, 
          and the task generator for generating questions about rotating objects' attributes. 
          The bottom part depicts the model evaluation process, which selects the relevant tasks based on the user's query and their budget and 
          performs either full evaluation or results approximation to answer the query.
        </div>
      </div>
    </div>
  </section>
  <br>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop is-centered has-text-centered">
        <h2 class="title is-3">Application 1: </h2>
        <h2 class="subtitle has-text-left">
          
        <!-- <div class="content has-text-justified">
        <img src="static/images/website_examples_1.jpg">
        <img src="static/images/website_examples_2.jpg">
        <div class="content has-text-justified"> -->
        <img src="./static/images/generation_process.png">
        <h2 class="subtitle has-text-left">
          <b>A benchmark generation engine that generates benchmarks on-the-fly tailored to the user’s need for assessing multimodal language models like GPT-4o.</b>
          The top part illustrates the task generation process with an example video synthesized with 3D objects and their annotations, 
          and the task generator for generating questions about rotating objects' attributes. 
          The bottom part depicts the model evaluation process, which selects the relevant tasks based on the user's query and their budget and 
          performs either full evaluation or results approximation to answer the query.
        </div>
      </div>
    </div>
  </section>
  <br>
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop is-centered has-text-centered">
        <h2 class="title is-3">Application 2: </h2>
        <h2 class="subtitle has-text-left">
          
        <!-- <div class="content has-text-justified">
        <img src="static/images/website_examples_1.jpg">
        <img src="static/images/website_examples_2.jpg">
        <div class="content has-text-justified"> -->
        <img src="./static/images/generation_process.png">
        <h2 class="subtitle has-text-left">
          <b>A benchmark generation engine that generates benchmarks on-the-fly tailored to the user’s need for assessing multimodal language models like GPT-4o.</b>
          The top part illustrates the task generation process with an example video synthesized with 3D objects and their annotations, 
          and the task generator for generating questions about rotating objects' attributes. 
          The bottom part depicts the model evaluation process, which selects the relevant tasks based on the user's query and their budget and 
          performs either full evaluation or results approximation to answer the query.
        </div>
      </div>
    </div>
  </section>
  <br>
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop is-centered has-text-centered">
        <h2 class="title is-3">Application 3: </h2>
        <h2 class="subtitle has-text-left">
          
        <!-- <div class="content has-text-justified">
        <img src="static/images/website_examples_1.jpg">
        <img src="static/images/website_examples_2.jpg">
        <div class="content has-text-justified"> -->
        <img src="./static/images/generation_process.png">
        <h2 class="subtitle has-text-left">
          <b>A benchmark generation engine that generates benchmarks on-the-fly tailored to the user’s need for assessing multimodal language models like GPT-4o.</b>
          The top part illustrates the task generation process with an example video synthesized with 3D objects and their annotations, 
          and the task generator for generating questions about rotating objects' attributes. 
          The bottom part depicts the model evaluation process, which selects the relevant tasks based on the user's query and their budget and 
          performs either full evaluation or results approximation to answer the query.
        </div>
      </div>
    </div>
  </section>
  <br>










  <section class="section is-light" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{zhang2024task,
  title={Generate Any Scene},
  author={Zhang, Jieyu and Huang, Weikai and Ma, Zixian and Michel, Oscar and He, Dong and Gupta, Tanmay and Ma, Wei-Chiu and Farhadi, Ali and Kembhavi, Aniruddha and Krishna, Ranjay},
  journal={arXiv preprint arXiv:2406.11775},
  year={2024}
}

</code></pre>


    </div>
  </section>
  <br>
  <br>
  <br>




  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>