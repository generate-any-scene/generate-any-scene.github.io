<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="TaskMeAnything">
  <meta name="keywords" content="Multimodal, Large Language Model, benchmark, synthetic data, dataset">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>TaskMeAnything</title>

  <!-- Google Tag Manager -->
  <script>(function (w, d, s, l, i) {
      w[l] = w[l] || []; w[l].push({
        'gtm.start':
          new Date().getTime(), event: 'gtm.js'
      }); var f = d.getElementsByTagName(s)[0],
        j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
          'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    })(window, document, 'script', 'dataLayer', 'GTM-NKDF32ZZ');</script>
  <!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <!-- MathJax -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
  <!-- Google Tag Manager (noscript) -->
  <!-- <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NKDF32ZZ" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript> -->
  <!-- End Google Tag Manager (noscript) -->

  <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://yushi-hu.github.io">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://zeyofu.github.io/blink/">
              BLINK
            </a>
            <a class="navbar-item" href="https://visual-program-distillation.github.io/">
              Visual Program Distillation (VPD)
            </a>
            <a class="navbar-item" href="https://finegrainedrlhf.github.io/">
              Fine-Grained RLHF
            </a>
            <a class="navbar-item" href="https://tifa-benchmark.github.io/">
              TIFA
            </a>
            <a class="navbar-item" href="https://yushi-hu.github.io/promptcap_demo/">
              PromptCap
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav> -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- <h1 class="title is-1 publication-title"><img src="static/images/icon.png" alt="icon"
                style="width:75px; transform: translateY(20%);"><span style="color: rgb(100, 138, 195);"><b>Task Me Anything</b></span>: <br> Sketching as a Visual Chain of Thought for Multimodal Language Models</h1> -->
            <h1 class="title is-1 publication-title">
                <span style="color: rgb(100, 138, 195);"><b>Task Me Anything</b></span></h1>
            <div class="is-size-5">
              <span class="author-block">
                <a href="https://jieyuz2.github.io/" style="color:#f68946;font-weight:normal;">Jieyu Zhang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://weikaih04.github.io/" style="color:#f68946;font-weight:normal;">Weikai Huang<sup>*</sup></a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://zixianma.github.io/" style="color:#f68946;font-weight:normal;">Zixian Ma<sup>*</sup></a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://ojmichel.github.io/" style="color:#008AD7;font-weight:normal;">Oscal Michel</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://dongheuw.github.io/" style="color:#f68946;font-weight:normal;">Dong He</a><sup>1</sup>,
              </span>
              <br>
              <span class="author-block">
                <a href="https://tanmaygupta.info/" style="color:#008AD7;font-weight:normal;">Tanmay Gupta</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://people.csail.mit.edu/weichium/" style="color:#008AD7;font-weight:normal;">Wei-Chiu Ma</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://homes.cs.washington.edu/~ali/" style="color:#f68946;font-weight:normal;">Ali Farhadi</a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a href="https://anikem.github.io/" style="color:#008AD7;font-weight:normal;">Aniruddha Kembhavi</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="http://www.ranjaykrishna.com/" style="color:#f68946;font-weight:normal;">Ranjay Krishna</a><sup>1,2</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <b style="color:#f68946; font-weight:normal">▶ </b>University of Washington
              </span>
              <span class="author-block">
                <b style="color:#008AD7; font-weight:normal">▶ </b>Allen Institute for AI
              </span>
              <!-- <span class="author-block">
                <b style="color:#008AD7; font-weight:normal">▶ </b>University of Pennsylvania
              </span> -->
              <br>
              <span class="author-block">
                &nbsp;&nbsp;<sup>*</sup>Equal Contribution
              </span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- PDF Link. -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/pdf/2406.09403" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span> -->
                <!-- Video Link. -->

                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/JieyuZ2/TaskMeAnything"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
  </section>




  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop is-centered has-text-centered">
        <img src="./static/images/teaser.png" alt="Teaser Image" style="height: 60%; display: block; margin-left: auto; margin-right: auto;">
        <br>
        <h2 class="subtitle has-text-left">
          <span style="display: inline-block; width: 1em;">
          </span>This paper introduces Task-Me-Anything, a benchmark generation engine
          which produces a benchmark tailored to a user’s needs. Task-Me-Anything
          maintains an extendable taxonomy of visual assets and can programmatically generate
          a vast number of task instances. Additionally, it algorithmically addresses
          user queries regarding MLM performance efficiently within a computational budget.
          It contains 113K images, 10K videos, 2K 3D object assets, over 365 object
          categories, 655 attributes, and 335 relationships. It can generate 750M image/video
          question-answering pairs, which focus on evaluating MLM perceptual capabilities.
          Task-Me-Anything reveals critical insights: open-source MLMs excel in object
          and attribute recognition but lack spatial and temporal understanding; each model
          exhibits unique strengths and weaknesses; larger models generally perform better,
          though exceptions exist; and GPT-4o demonstrates challenges in recognizing
          rotating/moving objects and distinguishing colors.
        <div class="content has-text-justified">
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop is-centered has-text-centered">
        <h2 class="title is-3">What is Task Me Anything?</h2>
        <h2 class="subtitle has-text-left">
          A benchmark generation engine that generates benchmarks on-the-fly tailored to the user’s need for assessing multimodal language models like GPT-4o
        <!-- <div class="content has-text-justified">
        <img src="static/images/website_examples_1.jpg">
        <img src="static/images/website_examples_2.jpg">
        <div class="content has-text-justified"> -->
        <img src="./static/images/generation_process.png">
        <h2 class="subtitle has-text-left">
          The top part illustrates the task generation process with an example video synthesized with 3D objects and their annotations, 
          and the task generator for generating questions about rotating objects' attributes. 
          The bottom part depicts the model evaluation process, which selects the relevant tasks based on the user's query and their budget and 
          performs either full evaluation or results approximation to answer the query.  
        </div>
      </div>
    </div>
  </section>



  <section class="hero is-small">
    <div class="container is-max-desktop is-centered">
      <div class="has-text-centered">
        <h2 class="title is-3">What are in Task Me Anything?</h2>
      </div>
      <br>
      <div style="text-align: left;">
        <h2 class="subtitle">
        <ul style="list-style-position: inside; padding-left: 20px;">
          <li>365 object categories, 655 attributes, 335 relationships</li>
          <li>28 task generators</li>
          <li>Can possibly generate over 750M ImageQA / VideoQA tasks</li>
          <li>Support fine-grained user queries with on-budget results approximation
            <ul style="list-style-position: inside; padding-left: 20px;">
              <li>Top-Ｋ: Find Top-5 objects that GPT4o is worst at recognizing when rotating</li>
              <li>Threshold: Identify colors that GPT-4o recognizes with less than 30% average accuracy</li>
              <li>Model Comparison: Compare with LLaVA-Next to determine which objects GPT-4o performs better at recognizing.</li>
              <li>Model Debugging: Identify types of animals for which all open-source models perform significantly worse compared to other animals</li>
            </ul>
          </li>
        </ul>
        </h2>
      </div>
    </div>
  </section>


  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop is-centered has-text-centered">
        <h2 class="title is-3">The task space of Task Me Anything</h2>
        <img src="./static/images/stats.png">
        <h2 class="subtitle has-text-left">
          The statistics of generatable tasks of each task generator and example image/video in Task-Me-Anything. 
          We roughly each task generator with high-level perceptual skills and this collection of task generators can collectively generate over 750M VQA tasks. 
          The task space of Task Me Anything is easy to grow exponentially by (1) adding new source data (eg, 3D object models) or (2) adding new task generators. 
          We plan to continuously expand the task space to adapt to the ever-changing capability of MLMs 
        </div>
      </div>
    </div>
  </section>


  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop is-centered has-text-centered">
        <h2 class="title is-3">Analysis</h2>
        <h2 class="title is-5">Query 1: How do models perform over a random subset of all possible questions?</h2>
        <img src="./static/images/analysis/query1-1.png">
        <h2 class="subtitle has-text-left">
          We evaluated 18 MLMs on the Task-Me-Anything-Random, a random set of generated tasks, with a detailed prompt and a succinct prompt.
          The detailed prompt typically yields better results; however, certain models, like GPT4V, perform much better with the succinct prompt, 
          indicating that current models are still prompt-sensitive. For ImageQA tasks, the latest open-sourced models, such as InternVL-Chat-1.5-24B and LLaVA-NEXT-34B, 
          perform better than popular proprietary models, achieving state-of-the-art performance. 
          Notably, models like InstructBlip-7B and Qwen-VL perform significantly better with detailed prompt than succinct prompt. 
          For VideoQA tasks, we also evaluated larger or proprietary ImageQA models, like GPT4V, by concatenating four frames of a video into a single picture. 
          Notably, Video-LLaVA-7B perform much better with succinct prompts than other small open-source models. 
        </h2>
        <br>
        <h2 class="title is-5">Query 2: What skills are MLMs best and worst at?</h2>
        <h2 class="subtitle has-text-left">
          We analyze performance across different perceptual capabilities to answer: what skills are all models good or bad at? 
          We conduct this study for both ImageQA and VideoQA tasks respectively. 
          We find that no specific skill appears to be the best or worst across (both image and video) models. 
          We see that all models struggle in spatial reasoning, counting objects, and 3D attribute understanding on ImageQA tasks, 
          and object recognition, temporal understanding on VideoQA tasks. 
          They perform well on object, attribute, and other relationship recognition instances.
          Surprisingly, we find that most MLMs perform the best at relationship understanding between objects, 
          scoring high if not perfectly on interactional relations such as ``riding'', ``looking into'', ``lying next to'' etc. 
          On the other hand, these models struggle the most in spatial reasoning in synthetic images, performing poorly especially on 
          questions that ask about objects in the ``middle'', ``bottom'' or ``back'' (for 3D images) part of the image. 
          Nevertheless, some models behave differently. For example, LLaVA-13B is worst at recognizing 3D attributes, 
          failing at identifying the ``smallest'' or ``closest'' 3D objects correctly. 
          Meanwhile, LLaVA-7B is best at object recognition and worst at relation understanding, 
          struggling to understand simple actions such as ``touching'' that other models perform well on.
        </h2>
        <br>
        <h2 class="title is-5">Query 3: What is the best MLM for each specific skill?</h2>
        <img src="./static/images/analysis/query3-1.png">
        <h2 class="subtitle has-text-left">
          LLaVA-13B stood out as the strongest model on ImageQA tasks, achieving the best performance on all skills except for relation understanding; 
          and Video-LLaVA-7B is the overall winner on VideoQA tasks, scoring the highest on action understanding and second or third elsewhere. 
          Specifically, we find that LLaVA-13B performs consistently better than other multi-modal models on all skills except for relation understanding, 
          where Qwen-VL-Chat performs better \((a)\). On VideoQA tasks, in addition to Video-LLaVA-7B, Chat-Univi-7B is also relatively well-rounded, 
          positioning in the top 3 models across all skills except for Attribute understanding  \((b)\). On the other hand, while VideoChat2-7B specializes in object, attribute, 
          and temporal attribute understanding, it falls short on Action and Relation reasoning
          More analysis on finer-grained skills can be found in the paper.
        </h2>
        <br>
        <h2 class="title is-5">Query 4: How do small models compare against large models?</h2>
        <img src="./static/images/analysis/query5-1.png">
        <h2 class="subtitle has-text-left">
          We are also interested in the relative performance of small versus large models with the same skills. On ImageQA tasks, for example, we observe that large multi-modal models collectively perform better than smaller models on ImageQA tasks. 
          Nevertheless, this finding might not always hold for individual models. Through t-tests with pairs of small and large models from the same source, 
          we find one exception: InstructBlip-7B \((\mu = 0.63)\) significantly outperforms InstructBlip-13B \((\mu = 0.49)\) on relation understanding (with \(p\)-value \(< 1e-5\)). 
        </h2>
        <br>
        <h2 class="title is-5">Query 5: What is today's popular proprietary model: GPT-4o bad at?</h2>
        <img src="./static/images/analysis/query7-1.png">
        <h2 class="subtitle has-text-left">
          Finally, we investigate GPT-4o, today's popular proprietary model:
          1) what \(\emph{objects}\) are GPT-4o bad at recognizing when rotating/moving?
          2) what \(\emph{relations}\) are GPT-4o bad at understanding?
          3) what \(\emph{attributes}\) of objects are GPT-4o bad at recognizing?
          To answer these questions, we first identify task generators for each question that can generate relevant tasks to evaluate, based on which we compare GPT-4o's performance across different coarse-grained object/relation/attribute categories and their average. We can see that
          1) GPT-4o does not perform well in recognizing “interactional” relations in images and “spatial” relations in videos, 
          2) recognizing rotating/moving “furniture”, “food”, and “plant” is more challenging for GPT-4o than other object categories such as animal and vehicle, 
          3) GPT-4o is worse at recognizing “color” than other attributes.
          Analysis on fine-grained objects/relations/attributes that GPT-4o is bad at can be found in the paper.
        </h2>
        <div class="content has-text-justified">
        </div>
      </div>
    </div>
  </section>


  <section class="hero is-small">
    <div class="container is-max-desktop content">
      <br>
      <h2 class="title is-3">Related Work</h2>
      <ul>
        <li> <a href="https://prior.allenai.org/projects/visprog">Visual Programming for Compositional Visual
            Reasoning</a></li>
        <li> <a href="https://viper.cs.columbia.edu/">ViperGPT: Visual Inference via Python Execution for
            Reasoning</a></li>
        <li> <a href="https://visual-program-distillation.github.io/">Visual Program Distillation:
            Distilling Tools and Programmatic Reasoning into Vision-Language Models</a>
        </li>
        <li> <a href="https://arxiv.org/abs/2210.03629">ReAct: Synergizing Reasoning and Acting in Language
            Models</a>
        </li>
        <li> <a href="https://github.com/microsoft/autogen">AutoGen</a>
        </li>
        <li> <a href="https://zeyofu.github.io/blink/">BLINK: Multimodal Large Language Models Can See but
            Not Perceive</a>
        </li>
        <li> <a href="https://vstar-seal.github.io/">V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs</a>
        </li>
        <li> <a href="https://arxiv.org/abs/2401.06209v1">Eyes Wide Shut? Exploring the Visual Shortcomings of
            Multimodal LLMs</a>
        </li>
        <li> <a href="https://isobench.github.io/">IsoBench: Benchmarking Multimodal Foundation Models on
            Isomorphic Representations</a>
        </li>
        <li> <a href="https://arxiv.org/abs/2105.04165v3">Inter-GPS: Interpretable Geometry Problem Solving
            with Formal Language and Symbolic Reasoning</a>
        </li>
      </ul>
    </div>
  </section> -->

  <section class="section is-light" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <!-- <pre><code>@article{hu2024visual,
title={Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models},
author={Hu, Yushi and Shi, Weijia and Fu, Xingyu and Roth, Dan and Ostendorf, Mari and Zettlemoyer, Luke and Smith, Noah A and Krishna, Ranjay},
journal={arXiv preprint arXiv:2406.09403},
year={2024}
}
</code></pre> -->
    </div>
  </section>





  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>