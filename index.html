<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="GenerateAnyScene">
  <meta name="keywords" content="Multimodal, Large Language Model, benchmark, synthetic data, dataset, Diffusion model, Text-to-image model">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Generate Any Scene</title>

  <!-- Google Tag Manager -->
  <script>(function (w, d, s, l, i) {
      w[l] = w[l] || []; w[l].push({
        'gtm.start':
          new Date().getTime(), event: 'gtm.js'
      }); var f = d.getElementsByTagName(s)[0],
        j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
          'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    })(window, document, 'script', 'dataLayer', 'GTM-NKDF32ZZ');</script>
  <!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <!-- MathJax -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.16.0/gradio.js"></script>
</head>

<body>
  <!-- Google Tag Manager (noscript) -->
  <!-- <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NKDF32ZZ" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript> -->
  <!-- End Google Tag Manager (noscript) -->

  <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://yushi-hu.github.io">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://zeyofu.github.io/blink/">
              BLINK
            </a>
            <a class="navbar-item" href="https://visual-program-distillation.github.io/">
              Visual Program Distillation (VPD)
            </a>
            <a class="navbar-item" href="https://finegrainedrlhf.github.io/">
              Fine-Grained RLHF
            </a>
            <a class="navbar-item" href="https://tifa-benchmark.github.io/">
              TIFA
            </a>
            <a class="navbar-item" href="https://yushi-hu.github.io/promptcap_demo/">
              PromptCap
            </a>
          </div>
        </div>
      </div>

    </div>
    </nav> -->



  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- <h1 class="title is-1 publication-title"><img src="static/images/icon.png" alt="icon"
                style="width:75px; transform: translateY(20%);"><span style="color: rgb(100, 138, 195);"><b>Task Me Anything</b></span>: <br> Sketching as a Visual Chain of Thought for Multimodal Language Models</h1> -->
            <h1 class="title is-1 publication-title"><img src="static/images/icon.png" alt="icon"
                style="width:75px; transform: translateY(20%);"><span style="color: rgb(15, 46, 93);"><b>Generate Any Scene
                  </b></span>: <br> Evaluating and Improving Text-to-Vision Generation with Scene Graph Programming</h1>
            <div class="is-size-5">
              <span class="author-block">
                <a href="https://uwGZQ.github.io/" style="color:#E86254;font-weight:normal;">Ziqi Gao<sup>*</sup></a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://weikaih04.github.io/" style="color:#E86254;font-weight:normal;">Weikai Huang<sup>*</sup></a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://jieyuz2.github.io/" style="color:#E86254;font-weight:normal;">Jieyu Zhang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://anikem.github.io/" style="color:#384995;font-weight:normal;">Aniruddha Kembhavi</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="http://www.ranjaykrishna.com/" style="color:#E86254;font-weight:normal;">Ranjay Krishna</a><sup>1,2</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <b style="color:#E86254; font-weight:normal">‚ñ∂ </b>University of Washington
              </span>
              <span class="author-block">
                <b style="color:#384995; font-weight:normal">‚ñ∂ </b>Allen Institute for AI
              </span>
              <!-- <span class="author-block">
                <b style="color:#008AD7; font-weight:normal">‚ñ∂ </b>University of Pennsylvania
              </span> -->
              <span class="author-block">
                &nbsp;&nbsp;<sup>*</sup>Equal Contribution
              </span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2412.08221" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                
                <!-- PDF Link. -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/pdf/2406.09403" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span> -->
                <!-- Video Link. -->

                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/RAIVNLab/GenerateAnyScene"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- <span class="link-block">
                  <a href="https://huggingface.co/spaces/zixianma/TaskMeAnything-UI"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Graphical Interface</span>
                  </a>
                </span> -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/UWGZQ/GenerateAnyScene"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        ü§ó
                    </span>
                    <span>Caption Dataset</span>
                  </a>
                </span> 
              </div>
            </div>
          </div>
        </div>
      </div>
  </section>

  <section class="hero is-light" style="background-color: #f0f0f0;">
    <div class="hero-body">
        <div class="container is-max-desktop has-text-centered">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-2">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Generative models like DALL-E and Sora have gained attention by producing implausible images, such as ‚Äúastronauts riding a horse in space.‚Äù Despite the proliferation of text-to-vision models that have inundated the internet with synthetic visuals, from images to 3D assets, current benchmarks predominantly evaluate these models on real-world scenes paired with captions.
                            We introduce <strong style="font-weight: bold;">Generate Any Scene</strong>, a groundbreaking framework that systematically enumerates scene graphs representing a vast array of visual scenes, spanning realistic to imaginative compositions. Generate Any Scene leverages <em style="font-weight: 500">Scene Graph Programming</em>: a revolutionary method for dynamically constructing scene graphs of varying complexity from a structured taxonomy of visual elements. This taxonomy includes numerous objects, attributes, and relations, enabling the synthesis of an almost infinite variety of scene graphs. Using these structured representations, Generate Any Scene translates each scene graph into a caption, enabling scalable evaluation of text-to-vision models through standard metrics.
                            We conduct extensive evaluations across multiple text-to-image, text-to-video, and text-to-3D models, presenting key findings on model performance. We find that DiT-backbone text-to-image models align more closely with input captions than UNet-backbone models. Text-to-video models struggle with balancing dynamics and consistency, while both text-to-video and text-to-3D models show notable gaps in human preference alignment.
                            Additionally, we demonstrate the effectiveness of Generate Any Scene by conducting three practical applications leveraging captions generated by Generate Any Scene:
                            <ol style="margin-left: 40px;">
                                <li>A <span style="color:#384995; font-weight: bold;">self-improving framework</span> where models iteratively enhance their performance using generated data.</li>
                                <li>A <span style="color:#588169; font-weight: bold;">distillation process</span> to transfer specific strengths from proprietary models to open-source counterparts.</li>
                                <li>Improvements in <span style="color:#E86254; font-weight: bold;">content moderation</span> by identifying and generating challenging synthetic data.</li>
                            </ol>
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </div>
  </section>

    <section class="hero is-small">
        <div class="hero-body">
            <div class="container is-max-desktop is-centered">
                <h2 class="title is-2">Scene Graph Programming</h2>
                <h3 class="title is-4">Metadata</h3>

                <b>To construct a scene graph, we use three main metadata types: 28,787 <strong style="font-weight: bold;">objects</strong>, 1,494 <strong style="font-weight: bold;">attributes</strong>, and 10,492 <strong style="font-weight: bold;">Relationships</strong>. We also have 2,193 <strong style="font-weight: bold;">scene attributes</strong> that capture the board aspect of the caption, such as art style, to create a complete visual caption.</b>
                <div class="latex-table" style="margin-top: 20px; text-align: center;">
                    <b>
                        $$\begin{array}{lll}
                        \hline
                        \textbf{Metadata Type} & \textbf{Number} & \textbf{Source} \\
                        \hline
                        \text{Objects} & 28,787 & \text{WordNet} \\
                        \text{Attributes} & 1,494 & \text{Wikipedia, etc.} \\
                        \text{Relations} & 10,492 & \text{Robin} \\
                        \text{Scene Attributes} & 2,193 & \text{Places365, etc.} \\
                        \hline
                        \end{array}$$
                    </b>
                </div>
                <h3 class="title is-4">Captions Generation Process</h3>
                <img src="./static/images/arch.png">
                <b>The generation pipeline of Generate Any Scene:</b>
                    <ui>

                    <li><b><strong>Step 1</strong>: The system enumerates scene graph structures that contain objects, attributes, and relations based on complexity, and queries the corresponding scene graph structure that satisfies the needs.</b></li>
                    <li><b><strong>Step 2</strong>: It populates these structures with metadata, assigning specific content to each node. Scene graphs are completed in this step. </b></li>
                    <li><b><strong>Step 3</strong>: In addition to the scene graph, scene attributes‚Äîsuch as art style and camera settings‚Äîare sampled to provide contextual depth beyond the scene graph. </b></li>
                    <li><b><strong>Step 4</strong>: The Generate Any Scene system combines the scene graph and scene attributes, such as art style and camera settings, and then translates them into a coherent caption by organizing the elements into structured text.</b></li>
                    </ui>
                
                <!-- <h3 class="subtitle is-3">Step 1</h3> -->

            </div>
        </div>
    </section>
    <section class="hero is-small">
        <div class="hero-body">
            <div class="container is-max-desktop is-centered">
            <h2 class="title is-2">Overall Results</h2>

          <div class="columns is-vcentered">
            <!-- Â∑¶‰æßÂõæÁâá -->
            <div class="column is-half" style="padding-top: 60px; padding-bottom: 0px;">
              <!-- <figure class="image is-4by3"> -->
                <img src="./static/images/image_overall.png" alt="Example Image">
                <figcaption style="text-align: center; margin-bottom: 15px; ">
                    Comparative evaluation of text-to-image models across different backbones (DiT and UNet) using multiple metrics: TiFA-Score, Pick-Score, VQA-Score, and Image-Reward-Score (Evaluated on 10K <span class="generate-any-scene">Generate Any Scene</span> Captions).
                  </figcaption>
              <!-- </figure> -->
            </div>
      
            <!-- Âè≥‰æßË°®Ê†º -->
            <div class="column is-half">
                <img src="./static/images/video_overall.png" alt="Video Result (overall)">
                <figcaption style="text-align: center; margin-bottom: 15px; margin-top: 0px">
                    Overall performance of text-to-video models on 10K <span class="generate-any-scene">Generate Any Scene</span> captions.
                    <span style="background-color: #FFCCCC;">Red Cell</span> is the highest score. <span style="background-color: #FFFFCC;">Yellow Cell</span> is the second highest score. 
                  </figcaption>
                  <img src="./static/images/video_vbench.png" alt="Video Result (Vbench)">
                  <figcaption style="text-align: center; margin-bottom: 15px;">
                      Overall performance of text-to-video models on 10K <span class="generate-any-scene">Generate Any Scene</span> captions with VBench metrics. 
                      <span style="background-color: #FFCCCC;">Red Cell</span> is the highest score.  <span style="background-color: #ccddff;">Blue Cell</span> is the lowest score.
                    </figcaption>
                    <img src="./static/images/3d_overall.png" alt="Video Result (Vbench)">
                    <figcaption style="text-align: center; margin-bottom: 20px;">
                      Overall performance of text-to-3D models on 10K <span class="generate-any-scene">Generate Any Scene</span> captions with VBench metrics. 
                  </figcaption>
                  
            </div>
          </div>

        </div>
    </div>
      </section>





  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop is-centered">
        <h2 class="title is-2">Application</h2>
        <!-- <h2 class="subtitle has-text-left"> -->
          
        <!-- <div class="content has-text-justified">
        <img src="static/images/website_examples_1.jpg">
        <img src="static/images/website_examples_2.jpg">
        <div class="content has-text-justified"> -->
        <img src="./static/images/app.png">
        <!-- <h3 class="title is-3"></h3> -->
            <!-- <h3 class="title is-3">Metadata</h3>    -->
        <b>We propose three applications of <span class="generate-any-scene">Generate Any Scene</span>.</b>
            <ui>
            <li><b><span style="color:#384995;">Application 1 - (Self-improving)</span>: Iteratively enhances a model by generating images with <span class="generate-any-scene">Generate Any Scene</span> captions, selecting the best, and fine-tuning, yielding a performance boost. </li>
            <li><b><span style="color:#588169;">Application 2 - (Distilling limitations)</span>: Distills strengths from proprietary models, such as better compositionality and hard concept understanding, into open-source models. </b></li>
            <li><b><span style="color:#E86254;">Application 3 - (Generated content detector)</span>: Robustify AI-generated content detection by training on diverse synthetic data generated by <span class="generate-any-scene">Generate Any Scene</span>‚Äòs captions.</b></li>
            </ui>
        
      </div>
    </div>
  </section>
  <br>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop is-centered">
        <h2 class="title is-4" style="color:#384995">Application 1: </h2>
        <!-- <h2 class="subtitle has-text-left"> -->
          
        <!-- <div class="content has-text-justified">
        <img src="static/images/website_examples_1.jpg">
        <img src="static/images/website_examples_2.jpg">
        <div class="content has-text-justified"> -->
        <img src="./static/images/app1.png">
        <figcaption style="text-align: center; margin-bottom: 35px; font-weight: lighter;">
            Average VQA score of stable diffusion v1.5fine-tuned on different data across 1K \name image/video evaluation set and GenAI-Bench image/video benchmark <span class="generate-any-scene">Generate Any Scene</span> captions with VBench metrics. 
        </figcaption>
        <b>
        We show that our diverse captions can facilitate a framework to iteratively improve text-to-vision models using their own generations.
        Given a model, we generate multiple images, identify the highest-scoring one, and use it as new fine-tuning data to improve the model itself.
        We fine-tune stable diffusion v1.5 and achieve an average of 5% performance boost compared with original models, and this method is even better than fine-tuning with the same amount of real images and captions from the Conceptual Captions CC3M over different benchmarks.
        </b>
          <!-- <b>A benchmark generation engine that generates benchmarks on-the-fly tailored to the user‚Äôs need for assessing multimodal language models like GPT-4o.
          The top part illustrates the task generation process with an example video synthesized with 3D objects and their annotations, 
          and the task generator for generating questions about rotating objects' attributes. 
          The bottom part depicts the model evaluation process, which selects the relevant tasks based on the user's query and their budget and 
          performs either full evaluation or results approximation to answer the query.
            </b> -->
      </div>
    </div>
  </section>
  <br>


  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop is-centered">
        <h2 class="title is-4" style="color:#588169;">Application 2: </h2>
        <!-- <h2 class="subtitle has-text-left"> -->
          
        <!-- <div class="content has-text-justified">
        <img src="static/images/website_examples_1.jpg">
        <img src="static/images/website_examples_2.jpg">
        <div class="content has-text-justified"> -->
        <img src="./static/images/app2.png">
        <figcaption style="text-align: center; margin-bottom: 35px; font-weight: lighter;">
            Examples of images generated by Dalle-3, the original stable-diffusion v1.5, and the fine-tuned versions. The left four captions demonstrate fine-tuning with multi-object captions generated by <span class="generate-any-scene">Generate Any Scene</span> for better compositionality, while the right two columns focus on understanding hard concepts.  
        </figcaption>
        <b>
            Using our evaluations, we identify limitations in open-sourced models that their proprietary counterparts excel at.
            Next, we distill these specific capabilities from proprietary models.
            For example, Dalle-3 excels particularly in generating composite images with multiple parts. We distill this capability into stable-diffusion v1.5, effectively bridging the gap between Dalle-3 and stable diffusion v1.5.             
        </b>
        <!-- <h2 class="subtitle has-text-left"> -->
      </div>
    </div>
  </section>
  <br>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop is-centered">
        <h2 class="title is-4" style="color:#E86254;">Application 3: </h2>
          
        <!-- <div class="content has-text-justified">
        <img src="static/images/website_examples_1.jpg">
        <img src="static/images/website_examples_2.jpg">
        <div class="content has-text-justified"> -->
        <img src="./static/images/app3.png">
        <figcaption style="text-align: center; margin-bottom: 35px; font-weight: lighter;">
            Comparison of detection performance across different data scales using D<sup>3</sup> alone versus the combined D<sup>3</sup> + <span class="generate-any-scene">Generate Any Scene</span> training set in cross-model and cross-dataset scenarios.
        </figcaption>

        <b>
            Content moderation is a vital application, especially as text-to-vision models improve. 
            We identify which kinds of data content moderation models are bad at detecting, generate more of such content, and retrain the detectors. We train a ViT-T with our generated data and boost its detection capabilities across benchmarks.
        </b>
        <!-- <h2 class="subtitle has-text-left"> -->
      </div>
    </div>
  </section>
  <br>



  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop is-centered has-text-centered">
        <h2 class="title is-5">
        Please refer to the paper for more experiments, analysis, and takeaways!</h2>

        </div>
        </div>
    </div>
    </section>





  <section class="section is-light" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{gao2024generatesceneevaluatingimproving,
        title={Generate Any Scene: Evaluating and Improving Text-to-Vision Generation with Scene Graph Programming}, 
        author={Ziqi Gao and Weikai Huang and Jieyu Zhang and Aniruddha Kembhavi and Ranjay Krishna},
        year={2024},
        eprint={2412.08221},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2412.08221}, 
  }

</code></pre>


    </div>
  </section>
  <br>
  <br>
  <br>




  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>
</html>

