<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="TaskMeAnything">
  <meta name="keywords" content="Multimodal, Large Language Model, benchmark, synthetic data, dataset">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>TaskMeAnything</title>

  <!-- Google Tag Manager -->
  <script>(function (w, d, s, l, i) {
      w[l] = w[l] || []; w[l].push({
        'gtm.start':
          new Date().getTime(), event: 'gtm.js'
      }); var f = d.getElementsByTagName(s)[0],
        j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
          'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    })(window, document, 'script', 'dataLayer', 'GTM-NKDF32ZZ');</script>
  <!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <!-- Google Tag Manager (noscript) -->
  <!-- <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NKDF32ZZ" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript> -->
  <!-- End Google Tag Manager (noscript) -->

  <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://yushi-hu.github.io">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://zeyofu.github.io/blink/">
              BLINK
            </a>
            <a class="navbar-item" href="https://visual-program-distillation.github.io/">
              Visual Program Distillation (VPD)
            </a>
            <a class="navbar-item" href="https://finegrainedrlhf.github.io/">
              Fine-Grained RLHF
            </a>
            <a class="navbar-item" href="https://tifa-benchmark.github.io/">
              TIFA
            </a>
            <a class="navbar-item" href="https://yushi-hu.github.io/promptcap_demo/">
              PromptCap
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav> -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- <h1 class="title is-1 publication-title"><img src="static/images/icon.png" alt="icon"
                style="width:75px; transform: translateY(20%);"><span style="color: rgb(100, 138, 195);"><b>Task Me Anything</b></span>: <br> Sketching as a Visual Chain of Thought for Multimodal Language Models</h1> -->
            <h1 class="title is-1 publication-title">
                <span style="color: rgb(100, 138, 195);"><b>Task Me Anything</b></span></h1>
            <div class="is-size-5">
              <span class="author-block">
                <a href="https://jieyuz2.github.io/" style="color:#f68946;font-weight:normal;">Jieyu Zhang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://weikaih04.github.io/" style="color:#f68946;font-weight:normal;">Weikai Huang<sup>*</sup></a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://zixianma.github.io/" style="color:#f68946;font-weight:normal;">Zixian Ma<sup>*</sup></a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://ojmichel.github.io/" style="color:#008AD7;font-weight:normal;">Oscal Michel</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://dongheuw.github.io/" style="color:#f68946;font-weight:normal;">Dong He</a><sup>1</sup>,
              </span>
              <br>
              <span class="author-block">
                <a href="https://tanmaygupta.info/" style="color:#008AD7;font-weight:normal;">Tanmay Gupta</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://people.csail.mit.edu/weichium/" style="color:#008AD7;font-weight:normal;">Wei-Chiu Ma</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://homes.cs.washington.edu/~ali/" style="color:#f68946;font-weight:normal;">Ali Farhadi</a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a href="https://anikem.github.io/" style="color:#008AD7;font-weight:normal;">Aniruddha Kembhavi</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="http://www.ranjaykrishna.com/" style="color:#f68946;font-weight:normal;">Ranjay Krishna</a><sup>1,2</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <b style="color:#f68946; font-weight:normal">▶ </b>University of Washington
              </span>
              <span class="author-block">
                <b style="color:#008AD7; font-weight:normal">▶ </b>Allen Institute for AI
              </span>
              <!-- <span class="author-block">
                <b style="color:#008AD7; font-weight:normal">▶ </b>University of Pennsylvania
              </span> -->
              <br>
              <span class="author-block">
                &nbsp;&nbsp;<sup>*</sup>Equal Contribution
              </span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- PDF Link. -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/pdf/2406.09403" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span> -->
                <!-- Video Link. -->

                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/JieyuZ2/TaskMeAnything"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
  </section>




  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop is-centered has-text-centered">
        <img src="./static/images/teaser.png" alt="Teaser Image" style="height: 60%; display: block; margin-left: auto; margin-right: auto;">
        <br>
        <h2 class="subtitle has-text-left">
          <span style="display: inline-block; width: 1em;">
          </span>This paper introduces Task-Me-Anything, a benchmark generation engine
          which produces a benchmark tailored to a user’s needs. Task-Me-Anything
          maintains an extendable taxonomy of visual assets and can programmatically generate
          a vast number of task instances. Additionally, it algorithmically addresses
          user queries regarding MLM performance efficiently within a computational budget.
          It contains 113K images, 10K videos, 2K 3D object assets, over 365 object
          categories, 655 attributes, and 335 relationships. It can generate 750M image/video
          question-answering pairs, which focus on evaluating MLM perceptual capabilities.
          Task-Me-Anything reveals critical insights: open-source MLMs excel in object
          and attribute recognition but lack spatial and temporal understanding; each model
          exhibits unique strengths and weaknesses; larger models generally perform better,
          though exceptions exist; and GPT-4o demonstrates challenges in recognizing
          rotating/moving objects and distinguishing colors.
        <div class="content has-text-justified">
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop is-centered has-text-centered">
        <h2 class="title is-3">What is Task Me Anything?</h2>
        <h2 class="subtitle has-text-left">
          A benchmark generation engine that generates benchmarks on-the-fly tailored to the user’s need for assessing multimodal language models like GPT-4o
        <!-- <div class="content has-text-justified">
        <img src="static/images/website_examples_1.jpg">
        <img src="static/images/website_examples_2.jpg">
        <div class="content has-text-justified"> -->
        <img src="./static/images/generation_process.png">
        <h2 class="subtitle has-text-left">
          The top part illustrates the task generation process with an example video synthesized with 3D objects and their annotations, 
          and the task generator for generating questions about rotating objects' attributes. 
          The bottom part depicts the model evaluation process, which selects the relevant tasks based on the user's query and their budget and 
          performs either full evaluation or results approximation to answer the query.  
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-small">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">What are in Task Me Anything?</h2>
      <ul>
        <li>365 object categories, 655 attributes, 335 relationships</li>
        <li>28 task generators</li>
        <li>Can possibly generate over 750M ImageQA / VideoQA tasks</li>
        <li>Support fine-grained user queries with on-budget results approximation
          <ul>
            <li>Top-Ｋ: Top-5 objects that GPT4o is worst at recognizing when rotating</li>
            <li>Threshold: Top-5 objects that GPT4o is worst at recognizing when rotating</li>
            <li>Model Comparison: Top-5 objects that GPT4o is worst at recognizing when rotating</li>
            <li>Model Debugging: Top-5 objects that GPT4o is worst at recognizing when rotating</li>
          </ul>
        </li>
      </ul>
      </div>
    </div>
  </section>


  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop is-centered has-text-centered">
        <h2 class="title is-3">The task space of Task Me Anything</h2>
        <img src="./static/images/stats.png">
        <h2 class="subtitle has-text-left">
          The statistics of generatable tasks of each task generator and example image/video in \name. 
          We roughly each task generator with high-level perceptual skills and this collection of task generators can collectively generate over 750M VQA tasks. 
          The task space of Task Me Anything is easy to grow exponentially by (1) adding new source data (eg, 3D object models) or (2) adding new task generators. 
          We plan to continuously expand the task space to adapt to the ever-changing capability of MLMs 
        </div>
      </div>
    </div>
  </section>


  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop is-centered has-text-centered">
        <h2 class="title is-3">Analysis</h2>
        <h2 class="title is-5">Query 1: How do models perform over a random subset of all possible questions?</h2>
        <img src="./static/images/analysis/query1-1.png">
        <h2 class="subtitle has-text-left">
          We evaluated 18 MLMs on the Task-Me-Anything-Random, a random set of generated tasks, with a detailed prompt and a succinct prompt.
          The detailed prompt typically yields better results; however, certain models, like GPT4V, perform much better with the succinct prompt, 
          indicating that current models are still prompt-sensitive. For ImageQA tasks, the latest open-sourced models, such as \internvlchat and \llavanext, 
          perform better than popular proprietary models, achieving state-of-the-art performance. 
          Notably, models like \instructblips and \qwenvl perform significantly better with detailed prompt than succinct prompt. 
          For VideoQA tasks, we also evaluated larger or proprietary ImageQA models, like GPT4V, by concatenating four frames of a video into a single picture. 
          Notably, \videollavas perform much better with succinct prompts than other small open-source models. 
        </h2>
        <img src="./static/images/analysis/query3-1.png">
        <img src="./static/images/analysis/query3-2.png">
        <img src="./static/images/analysis/query4-1.png">
        <img src="./static/images/analysis/query4-2.png">
        <img src="./static/images/analysis/query4-3.png">
        <img src="./static/images/analysis/query5-1.png">
        <img src="./static/images/analysis/query5-2.png">
        <img src="./static/images/analysis/query5-3.png">
        <img src="./static/images/analysis/query5-4.png">
        <img src="./static/images/analysis/query5-5.png">
        <img src="./static/images/analysis/query6-1.png">
        <img src="./static/images/analysis/query7-1.png">
        <div class="content has-text-justified">
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop is-centered has-text-centered">
        <h2 class="title is-3">Stats</h2>
        <img src="static/images/website_examples_1.jpg">
        <img src="static/images/website_examples_2.jpg">
        <div class="content has-text-justified">
        </div>
      </div>
    </div>
  </section> -->

  <!-- <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop is-centered has-text-centered">
        <h2 class="title is-3">Task Me Anything 1.0 Random</h2>
        <img src="static/images/website_examples_1.jpg">
        <img src="static/images/website_examples_2.jpg">
        <div class="content has-text-justified">
        </div>
      </div>
    </div>
  </section> -->







  <!-- <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop is-centered has-text-centered">
        <h2 class="title is-3">Evaluation results</h2>
        <img src="static/images/website_examples_1.jpg">
        <img src="static/images/website_examples_2.jpg">
        <div class="content has-text-justified">

        </div>
      </div>
    </div>
  </section> -->


  <!-- <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop is-centered has-text-centered">
        <h2 class="title is-3">Effectiveness of Sketchpad</h2>
        <img src="static/images/sketchpad_table1.jpg">
        <img src="static/images/sketchpad_table2.jpg">
        <div class="content has-text-justified">
          Sketchpad substantially improves performance on all tasks over GPT-4-turbo and GPT-4o with no sketching.
          Specifically, GPT-4o with sketchpad sets a new state of the art on all tasks.
        </div>
      </div>
    </div>
  </section>




  <section class="hero is-small">
    <div class="container is-max-desktop content">
      <br>
      <h2 class="title is-3">Related Work</h2>
      <ul>
        <li> <a href="https://prior.allenai.org/projects/visprog">Visual Programming for Compositional Visual
            Reasoning</a></li>
        <li> <a href="https://viper.cs.columbia.edu/">ViperGPT: Visual Inference via Python Execution for
            Reasoning</a></li>
        <li> <a href="https://visual-program-distillation.github.io/">Visual Program Distillation:
            Distilling Tools and Programmatic Reasoning into Vision-Language Models</a>
        </li>
        <li> <a href="https://arxiv.org/abs/2210.03629">ReAct: Synergizing Reasoning and Acting in Language
            Models</a>
        </li>
        <li> <a href="https://github.com/microsoft/autogen">AutoGen</a>
        </li>
        <li> <a href="https://zeyofu.github.io/blink/">BLINK: Multimodal Large Language Models Can See but
            Not Perceive</a>
        </li>
        <li> <a href="https://vstar-seal.github.io/">V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs</a>
        </li>
        <li> <a href="https://arxiv.org/abs/2401.06209v1">Eyes Wide Shut? Exploring the Visual Shortcomings of
            Multimodal LLMs</a>
        </li>
        <li> <a href="https://isobench.github.io/">IsoBench: Benchmarking Multimodal Foundation Models on
            Isomorphic Representations</a>
        </li>
        <li> <a href="https://arxiv.org/abs/2105.04165v3">Inter-GPS: Interpretable Geometry Problem Solving
            with Formal Language and Symbolic Reasoning</a>
        </li>
      </ul>
    </div>
  </section> -->


  <section class="section is-light" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <!-- <pre><code>@article{hu2024visual,
title={Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models},
author={Hu, Yushi and Shi, Weijia and Fu, Xingyu and Roth, Dan and Ostendorf, Mari and Zettlemoyer, Luke and Smith, Noah A and Krishna, Ranjay},
journal={arXiv preprint arXiv:2406.09403},
year={2024}
}
</code></pre> -->
    </div>
  </section>





  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>